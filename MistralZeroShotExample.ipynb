{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "\n",
        "!wget https://antidote.cloud/f/ae5312aa983845c7abf1/?dl=1 -O llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "KZgbxsNgk6fA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42715860-57b8-4e50-beda-13d98c98bc45"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "--2026-01-15 14:16:44--  https://antidote.cloud/f/ae5312aa983845c7abf1/?dl=1\n",
            "Resolving antidote.cloud (antidote.cloud)... 193.30.122.219\n",
            "Connecting to antidote.cloud (antidote.cloud)|193.30.122.219|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://antidote.cloud/seafhttp/files/f44a2c96-b68f-4538-ac71-21c3a33a971a/llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl [following]\n",
            "--2026-01-15 14:16:45--  https://antidote.cloud/seafhttp/files/f44a2c96-b68f-4538-ac71-21c3a33a971a/llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl\n",
            "Reusing existing connection to antidote.cloud:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52058581 (50M) [application/octet-stream]\n",
            "Saving to: ‘llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl’\n",
            "\n",
            "llama_cpp_python-0. 100%[===================>]  49.65M  15.3MB/s    in 3.4s    \n",
            "\n",
            "2026-01-15 14:16:49 (14.6 MB/s) - ‘llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl’ saved [52058581/52058581]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MINCy7vtlP75",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e9a030-988b-45e5-9e72-e1f7ade31d04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.3.16) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.3.16) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.16)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.3.16) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.16) (3.0.3)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (0.1.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2026.1.4)\n",
            "env: HF_HUB_ENABLE_HF_TRANSFER=1\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/commands/download.py:141: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
            "Downloading 'Mistral-Nemo-Instruct-2407-Q5_K_M.gguf' to '.cache/huggingface/download/nPs0nIBuHPYaf12n7QiCpTH-YPM=.f2118506b57d31403cba021b476ceb95600cc278fbc2feeaaf884b6e64fa6ee5.incomplete'\n",
            "Mistral-Nemo-Instruct-2407-Q5_K_M.gguf: 100% 8.73G/8.73G [01:00<00:00, 144MB/s]\n",
            "Download complete. Moving file to Mistral-Nemo-Instruct-2407-Q5_K_M.gguf\n",
            "Mistral-Nemo-Instruct-2407-Q5_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "#!CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama_cpp_python\n",
        "!CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install /content/llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl\n",
        "!pip install huggingface_hub hf_transfer\n",
        "%env HF_HUB_ENABLE_HF_TRANSFER=1\n",
        "!huggingface-cli download bartowski/Mistral-Nemo-Instruct-2407-GGUF Mistral-Nemo-Instruct-2407-Q5_K_M.gguf --local-dir . --local-dir-use-symlinks False\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download bartowski/Mistral-Nemo-Instruct-2407-GGUF \\\n",
        "Mistral-Nemo-Instruct-2407-Q5_K_M.gguf \\--local-dir .--local-dir-use-symlinks False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOkKVY8gA-f1",
        "outputId": "c9182498-c060-4e6e-854b-e8658cef34c3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: huggingface-cli <command> [<args>]\n",
            "huggingface-cli: error: unrecognized arguments: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "llm = Llama(\n",
        "  model_path=\"./Mistral-Nemo-Instruct-2407-Q5_K_M.gguf\",\n",
        "  n_ctx=2048, # Context window (tokens)\n",
        "  n_gpu_layers=35, # All layers on GPU\n",
        "  n_threads=8, # CPU threads\n",
        "  n_batch=512, # Batch size\n",
        "  verbose=False,\n",
        "  seed=1234 # Reproducibility!\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmDtiOCZBDNt",
        "outputId": "e272a1dc-3199-4658-c772-6aef7ee8f9e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (1024000) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"[INST] What is the capital of Germany? [/INST]\"\n",
        "output = llm(prompt, max_tokens=50, temperature=0)\n",
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lELFTl45BjLB",
        "outputId": "442507b1-3014-48c5-be27-674fd1a32163"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of Germany is Berlin. It has been the capital since the reunification of Germany in 1990, when the former West German capital, Bonn, and the former East German capital, East Berlin, were united. Berlin is known\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import LlamaGrammar\n",
        "import json\n",
        "schema = {\n",
        "  \"type\": \"object\",\n",
        "  \"properties\": {\n",
        "    \"sentiment\": {\n",
        "      \"type\": \"string\",\n",
        "      \"enum\": [\"positive\", \"negative\", \"neutral\"]\n",
        "    },\n",
        "    \"stars\": {\n",
        "      \"type\": \"string\",\n",
        "      \"enum\": [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
        "    }\n",
        "  },\n",
        "  \"required\": [\"sentiment\", \"stars\"]\n",
        "}\n",
        "grammar = LlamaGrammar.from_json_schema(json.dumps(schema))"
      ],
      "metadata": {
        "id": "xOfXVCQDBkIg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = \"This product is absolutely terrible. Waste of money.\"\n",
        "prompt = f\"[INST] {review} [/INST]\"\n",
        "output = llm(prompt, max_tokens=50, temperature=0, grammar=grammar)\n",
        "result = json.loads(output[\"choices\"][0][\"text\"])\n",
        "print(result) # {\"sentiment\": \"negative\", \"stars\": \"1\"}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE4D-bT-BvDY",
        "outputId": "8de72338-887b-495a-b067-418a8cbc4be5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentiment': 'negative', 'stars': '1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "path = kagglehub.dataset_download(\n",
        "\"ashishkumarak/amazon-shopping-reviews-daily-updated\"\n",
        ")\n",
        "df = pd.read_csv(path + \"/amazon_reviews.csv\")\n",
        "results = []\n",
        "for idx, row in df.head(50).iterrows():\n",
        "  prompt = f\"[INST] {row['content']} [/INST]\"\n",
        "  output = llm(prompt, max_tokens=50, temperature=0, grammar=grammar)\n",
        "  result = json.loads(output[\"choices\"][0][\"text\"])\n",
        "  result[\"original_stars\"] = row[\"score\"]\n",
        "  results.append(result)\n",
        "\n",
        "results_df = pd.DataFrame(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDDe6tWgBwgA",
        "outputId": "ff9e51e5-feff-4350-f877-0a15aaba7a11"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'amazon-shopping-reviews-daily-updated' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schema = {\n",
        "  \"type\": \"object\",\n",
        "  \"properties\": {\n",
        "    \"sentiment\": {\n",
        "      \"type\": \"string\",\n",
        "      \"enum\": [\"positive\", \"negative\", \"neutral\"]\n",
        "    },\n",
        "    \"topic\": {\n",
        "      \"type\": \"string\",\n",
        "      \"enum\": [\"delivery\", \"price\", \"quality\", \"service\", \"other\"]\n",
        "    },\n",
        "    \"mentions_competitor\": {\n",
        "      \"type\": \"string\",\n",
        "      \"enum\": [\"yes\", \"no\"]\n",
        "    },\n",
        "    \"actionable\": {\n",
        "      \"type\": \"string\",\n",
        "      \"enum\": [\"yes\", \"no\"]\n",
        "    }\n",
        "  },\n",
        "  \"required\": [\"sentiment\", \"topic\", \"mentions_competitor\", \"actionable\"]\n",
        "}"
      ],
      "metadata": {
        "id": "XtJMb3w-B8td"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This product is good.\""
      ],
      "metadata": {
        "id": "KIn_OP8TCHnc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The delivery was incredibly fast and the product quality exceeded my expectations. Highly recommend!\""
      ],
      "metadata": {
        "id": "RhoVN51WFqoG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This product broke after only a week of use. Absolutely terrible quality and a waste of money. I'm very disappointed.\""
      ],
      "metadata": {
        "id": "7B4DrGV3GD-i"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"[INST] {text} [/INST]\""
      ],
      "metadata": {
        "id": "ht68VCo3DbpU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"[INST] You are analyzing customer reviews for an\n",
        "e-commerce platform. Classify the sentiment and star rating:\n",
        "Review: {text} [/INST]\"\"\""
      ],
      "metadata": {
        "id": "3AbH3cvyDd2_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"[INST] Classify customer reviews:\n",
        "Examples:\n",
        "- \"Love it! Perfect!\"-> positive, 5 stars\n",
        "- \"Terrible. Broke immediately.\"-> negative, 1 star\n",
        "- \"Decent product for the price\"-> neutral, 3 stars\n",
        "\n",
        "Now classify: {text} [/INST]\"\"\""
      ],
      "metadata": {
        "id": "CLmgDb1tDi9O"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in llm(prompt, max_tokens=100, temperature=0, grammar=grammar, stream=True):\n",
        "  print(chunk[\"choices\"][0][\"text\"], end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlJotOgnDstv",
        "outputId": "0544d9e8-4758-4b4f-9f00-eabf7fc35fa1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"sentiment\": \"positive\", \"stars\": \"5\"}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in llm(prompt, max_tokens=100, temperature=0, grammar=grammar, stream=True):\n",
        "  print(chunk[\"choices\"][0][\"text\"], end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozYG2eVUFRMy",
        "outputId": "8c383cdb-29ec-449f-8527-b6807d7af793"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"sentiment\": \"positive\", \"stars\": \"4\"}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in llm(prompt, max_tokens=100, temperature=0, grammar=grammar, stream=True):\n",
        "  print(chunk[\"choices\"][0][\"text\"], end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGLYgwkBFeMx",
        "outputId": "9bd732b8-fef5-4da7-fdb2-321257b1e3b3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"sentiment\": \"negative\", \"stars\": \"1\"}"
          ]
        }
      ]
    }
  ]
}