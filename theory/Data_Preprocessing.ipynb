{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrV0s7U522GIf3nR3SjNR8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mioyn/AdvDataProg/blob/main/theory/Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imputation\n",
        "\n",
        "Many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values.\n",
        "\n",
        "Set the missing values to some value(Zero, the mean, the median, etc.)\n",
        "\n",
        "```\n",
        "# Fill missing values using forward fill then backward fill\n",
        "df[required_cols] = df[required_cols].fillna(method='ffill').fillna(method='bfill')\n",
        "```\n",
        "Another way is to use Scikit-learn class:SimpleImputer\n"
      ],
      "metadata": {
        "id": "LITbzOZeg6Nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imp = SimpleImputer(strategy='mean') # median, most_frequent\n",
        "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
        "\n",
        "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
        "print(imp.transform(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pewZ5hAZl3zT",
        "outputId": "c7f74b44-ee89-4a3e-ff0c-5f74348b5f23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4.         2.        ]\n",
            " [6.         3.66666667]\n",
            " [7.         6.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Outlier Detection & Removal\n",
        "### Interquartile Range (IQR) Method\n",
        "```\n",
        "# Function to remove outliers using IQR\n",
        "def remove_outliers_iqr(df, column):\n",
        "  #Compute Q1 (25th percentile) and Q3 (75th percentile).\n",
        "  Q1 = df[column].quantile(0.25)\n",
        "  Q3 = df[column].quantile(0.75)\n",
        "\n",
        "  # calculate IQR\n",
        "  IQR = Q3 - Q1\n",
        "\n",
        "  # define lower and upper bounds - values outside is considerd as outliers\n",
        "  lower_bound = Q1 - 1.5 * IQR\n",
        "  upper_bound = Q3 + 1.5 * IQR\n",
        "  \n",
        "  # remove outliers\n",
        "  df_cleaned = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "  return df_cleaned\n",
        "\n",
        "# Remove outliers for column salary\n",
        "df = remove_outliers_iqr(df.copy(), 'salary')\n",
        "```\n",
        "\n",
        "# Encoding\n",
        "Scikit-Learn provides a OneHotEncoder class to convert categorical values into one-hot vectors\n",
        "Output of OneHotEncoder is sparse matrix. A sparse matrix contains mostly zeros. Internally it only stores the nonzero values and their positions. when you have more categories it will save plenty of memory and speed up computations.\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "housing_cat = housing_df[['ocean_proximity']]\n",
        "encoder = OneHotEncoder()\n",
        "cat_1hot = encoder.fit_transform(housing_cat)\n",
        "\n",
        "# get list of categories\n",
        "encoder.categories_\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "lR90RUQLl5Pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Scaling\n",
        "\n",
        "The MinMax Scaler is a data preprocessing technique used to transform numerical features in a dataset to a specific range, typically between 0 and 1. This is also commonly referred to as normalization. The core idea is to make the minimum value in a feature column equal to 0, the maximum value equal to 1, and all other values fall proportionally in between.\n",
        "\n",
        "This is achieved using a simple mathematical formula applied to each data point:\n",
        "\n",
        "  $X_{\\text{scaled}}=\\frac{X-X_{\\text{min}}}{X_{\\text{max}}-X_{\\text{min}}}$\n",
        "\n",
        "Where:\n",
        "  \n",
        "  * $X$ is the original data point.\n",
        "  * $X_{\\text{min}}$ is the minimum value for that feature in the dataset.\n",
        "  * $X_{\\text{max}}$ is the maximum value for that feature in the dataset.\n",
        "\n",
        "```\n",
        "from\n",
        "```\n",
        "\n",
        "###Real-Time Predictive Maintenance\n",
        "In a factory, sensors continuously monitor the condition of critical machinery (like a turbine) to predict mechanical failures before they happen.\n",
        "\n",
        "#### The Raw Data:\n",
        "Imagine the data stream includes three features collected every second:\n",
        "  * **Vibration Level:** Measured in meters per second squared $(m/s^2)$, typically a small range (e.g., 0.1 to 5.0).\n",
        "  * **Temperature:** Measured in degrees Celsius $(^{\\circ }C)$, with a range of operating temperatures $(e.g., 20{}^{\\circ }C - 200{}^{\\circ }C)$.\n",
        "  * **Energy Consumption:** Measured in Megawatt-hours (MWh), a large range (e.g., 10 MWh to 500 MWh).\n",
        "\n",
        "####The Problem Without Scaling:\n",
        "An algorithm that relies on distance metrics (like K-Nearest Neighbors to detect an anomaly) would be completely dominated by the Energy Consumption feature because its values (hundreds) are much larger than the Vibration Level values (single digits) or Temperature values (tens/hundreds). The model would essentially ignore minor, but critical, changes in vibration that signal an impending failure.\n",
        "\n",
        "#### Applying a Scaler (e.g., MinMax Scaler):\n",
        "To ensure all features contribute equally, a MinMaxScaler is applied: Vibration Level is scaled from its range of [0.1, 5.0] to [0, 1].Temperature is scaled from its range of [20, 200] to [0, 1].Energy Consumption is scaled from its range of [10, 500] to [0, 1].\n",
        "#### The Result:\n",
        "The scaled features are all on the same playing field. The machine learning model can now effectively analyze patterns across all three features simultaneously. A sudden spike in vibration, even though its raw value is small, will have an impact comparable to a spike in temperature or energy consumption in the scaled data, leading to faster and more accurate predictions of potential equipment failure.\n",
        "\n",
        "\n",
        "\n",
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "frJmA9hUEUnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rS-S80sGg5ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpbBuxEsaWHN"
      },
      "outputs": [],
      "source": []
    }
  ]
}