{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTwcJA/iusZDlWH/wPMu8Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mioyn/AdvDataProg/blob/main/theory/Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imputation\n",
        "\n",
        "Many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values.\n",
        "\n",
        "Set the missing values to some value(Zero, the mean, the median, etc.)\n",
        "\n",
        "```\n",
        "# Fill missing values using forward fill then backward fill\n",
        "df[required_cols] = df[required_cols].fillna(method='ffill').fillna(method='bfill')\n",
        "```\n",
        "Another way is to use Scikit-learn class:SimpleImputer\n"
      ],
      "metadata": {
        "id": "LITbzOZeg6Nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imp = SimpleImputer(strategy='mean')\n",
        "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
        "SimpleImputer()\n",
        "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
        "print(imp.transform(X))"
      ],
      "metadata": {
        "id": "pewZ5hAZl3zT",
        "outputId": "c7f74b44-ee89-4a3e-ff0c-5f74348b5f23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4.         2.        ]\n",
            " [6.         3.66666667]\n",
            " [7.         6.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Outlier Detection & Removal\n",
        "### Interquartile Range (IQR) Method\n",
        "```\n",
        "# Function to remove outliers using IQR\n",
        "def remove_outliers_iqr(df, column):\n",
        "  #Compute Q1 (25th percentile) and Q3 (75th percentile).\n",
        "  Q1 = df[column].quantile(0.25)\n",
        "  Q3 = df[column].quantile(0.75)\n",
        "\n",
        "  # calculate IQR\n",
        "  IQR = Q3 - Q1\n",
        "\n",
        "  # define lower and upper bounds - values outside is considerd as outliers\n",
        "  lower_bound = Q1 - 1.5 * IQR\n",
        "  upper_bound = Q3 + 1.5 * IQR\n",
        "  \n",
        "  # remove outliers\n",
        "  df_cleaned = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "  return df_cleaned\n",
        "\n",
        "# Remove outliers for column salary\n",
        "df = remove_outliers_iqr(df.copy(), 'salary')\n",
        "```\n",
        "# Feature Scaling\n",
        "# Encoding\n",
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "lR90RUQLl5Pu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rS-S80sGg5ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpbBuxEsaWHN"
      },
      "outputs": [],
      "source": []
    }
  ]
}