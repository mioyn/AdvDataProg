{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mioyn/AdvDataProg/blob/main/LiteratureReviewAssistantv4(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Literature Review Assistant\n",
        "\n",
        "**Abstract → Components → Search → Select → Related Work**\n",
        "\n",
        "Run cells in order.\n",
        "\n",
        "\n",
        "Licensed under MIT License!!!"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85fc0d26-06ee-4216-872b-d0327377e289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA A100-SXM4-40GB\n",
            "\u001b[94mInstalling dependencies...\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[94mDownloading model...\u001b[0m\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/commands/download.py:141: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
            "Downloading 'Mistral-Nemo-Instruct-2407-Q5_K_M.gguf' to '.cache/huggingface/download/nPs0nIBuHPYaf12n7QiCpTH-YPM=.f2118506b57d31403cba021b476ceb95600cc278fbc2feeaaf884b6e64fa6ee5.incomplete'\n",
            "Mistral-Nemo-Instruct-2407-Q5_K_M.gguf: 100% 8.73G/8.73G [00:22<00:00, 387MB/s]\n",
            "Download complete. Moving file to Mistral-Nemo-Instruct-2407-Q5_K_M.gguf\n",
            "Mistral-Nemo-Instruct-2407-Q5_K_M.gguf\n",
            "\u001b[92mSetup complete\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Setup\n",
        "GPU_TYPE = \"A100\"  # @param [\"A100\", \"T4\"]\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# CSS hack for word wrap\n",
        "def set_css():\n",
        "    display(HTML('<style>pre { white-space: pre-wrap; }</style>'))\n",
        "get_ipython().events.register('pre_run_cell', set_css)\n",
        "\n",
        "def cprint(text, color='white'):\n",
        "    colors = {'red': '91', 'green': '92', 'yellow': '93', 'blue': '94',\n",
        "              'pink': '95', 'teal': '96', 'grey': '90', 'white': '97'}\n",
        "    print(f'\\033[{colors.get(color, \"97\")}m{text}\\x1b[0m')\n",
        "\n",
        "!nvidia-smi --query-gpu=name --format=csv,noheader\n",
        "\n",
        "cprint(\"Installing dependencies...\", 'blue')\n",
        "\n",
        "if GPU_TYPE == \"A100\":\n",
        "    !wget https://antidote.cloud/f/29294c604b024f2eb1ff/?dl=1 -O llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl -q\n",
        "else:\n",
        "    !wget https://antidote.cloud/f/ae5312aa983845c7abf1/?dl=1 -O llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl -q\n",
        "\n",
        "!pip install ./llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl -q\n",
        "!pip install huggingface_hub pandas --quiet\n",
        "\n",
        "cprint(\"Downloading model...\", 'blue')\n",
        "!huggingface-cli download bartowski/Mistral-Nemo-Instruct-2407-GGUF \\\n",
        "    Mistral-Nemo-Instruct-2407-Q5_K_M.gguf \\\n",
        "    --local-dir . --local-dir-use-symlinks False\n",
        "\n",
        "cprint(\"Setup complete\", 'green')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Load Model and Utilities\n",
        "from llama_cpp import Llama\n",
        "import json, os, re, requests, time\n",
        "\n",
        "if GPU_TYPE == \"A100\":\n",
        "    llm = Llama(\n",
        "        model_path=\"./Mistral-Nemo-Instruct-2407-Q5_K_M.gguf\",\n",
        "        n_ctx=65536, n_gpu_layers=-1, n_batch=2048, n_ubatch=512,\n",
        "        flash_attn=True, use_mmap=True, use_mlock=True, verbose=False, seed=42\n",
        "    )\n",
        "else:\n",
        "    llm = Llama(\n",
        "        model_path=\"./Mistral-Nemo-Instruct-2407-Q5_K_M.gguf\",\n",
        "        n_ctx=32768, n_gpu_layers=-1, n_batch=1024,\n",
        "        use_mmap=True, verbose=False, seed=42\n",
        "    )\n",
        "cprint(f\"Model loaded ({GPU_TYPE})\", 'green')\n",
        "\n",
        "# Shared state\n",
        "STATE = {'abstract': '', 'components': [], 'paper_components': []}\n",
        "\n",
        "def generate_bibtex_key(authors, year, title):\n",
        "    \"\"\"Generate a BibTeX key from paper metadata.\"\"\"\n",
        "    if authors:\n",
        "        first_author = authors[0].split()[-1].lower()\n",
        "        first_author = re.sub(r'[^a-z]', '', first_author)\n",
        "    else:\n",
        "        first_author = 'unknown'\n",
        "    year_str = str(year) if year else '0000'\n",
        "    title_words = re.findall(r'\\b[A-Za-z]+\\b', title or 'paper')\n",
        "    title_word = ''\n",
        "    for w in title_words:\n",
        "        if w.lower() not in ['a', 'an', 'the', 'of', 'for', 'and', 'in', 'on', 'to', 'with']:\n",
        "            title_word = w.lower()[:8]\n",
        "            break\n",
        "    return f\"{first_author}{year_str}{title_word}\"\n",
        "\n",
        "def format_author_citation(authors):\n",
        "    \"\"\"Format as 'Smith et al.' or 'Smith'.\"\"\"\n",
        "    if not authors:\n",
        "        return \"Unknown\"\n",
        "    last_name = authors[0].split()[-1]\n",
        "    return f\"{last_name} et al.\" if len(authors) > 1 else last_name\n",
        "\n",
        "def generate_bibtex_entry(paper):\n",
        "    \"\"\"Generate full BibTeX entry.\"\"\"\n",
        "    key = paper.get('bibtex_key', 'unknown')\n",
        "    authors = paper.get('authors', ['Unknown'])\n",
        "    author_str = ' and '.join(authors)\n",
        "    doi = f\"\\n  doi = {{{paper['doi']}}},\" if paper.get('doi') else \"\"\n",
        "    return f\"\"\"@article{{{key},\n",
        "  title = {{{paper.get('title', 'Unknown')}}},\n",
        "  author = {{{author_str}}},\n",
        "  year = {{{paper.get('year', '')}}},\n",
        "  journal = {{{paper.get('venue', 'Unknown')}}},{doi}\n",
        "}}\"\"\"\n",
        "\n",
        "cprint(\"Utilities loaded\", 'green')"
      ],
      "metadata": {
        "id": "load_model",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "1f3336de-09b9-47e9-81de-97b7361c04fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>pre { white-space: pre-wrap; }</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (65536) < n_ctx_train (1024000) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mModel loaded (A100)\u001b[0m\n",
            "\u001b[92mUtilities loaded\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Enter Abstract\n",
        "\n",
        "ABSTRACT = \"\"\"The rapid classification of social media content during humanitarian crises is essential for effective disaster response. This paper presents a training-free, multimodal classification framework using zero-shot learning with vision-language models for crisis-related social media analysis.\"\"\"  # @param {type:\"string\"}\n",
        "\n",
        "STATE['abstract'] = ABSTRACT\n",
        "cprint(\"Abstract saved\", 'green')\n",
        "print(f\"\\n{ABSTRACT}\")"
      ],
      "metadata": {
        "id": "abstract",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "469c02d1-f220-4d38-bd33-e13570cb599b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>pre { white-space: pre-wrap; }</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mAbstract saved\u001b[0m\n",
            "\n",
            "The rapid classification of social media content during humanitarian crises is essential for effective disaster response. This paper presents a training-free, multimodal classification framework using zero-shot learning with vision-language models for crisis-related social media analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Identify Components\n",
        "\n",
        "prompt = f\"\"\"[INST] Identify 4-6 key research components from this abstract that would make good literature review subsection topics.\n",
        "\n",
        "Abstract: {STATE['abstract']}\n",
        "\n",
        "Rules:\n",
        "- Each component: 2-5 words\n",
        "- Focus on methods, domains, key concepts\n",
        "- These will become subsection headings (e.g., \"2.1 Social Media in Disaster Response\")\n",
        "- Be specific enough to find relevant papers\n",
        "\n",
        "Return ONLY a numbered list:\n",
        "1. First component\n",
        "2. Second component\n",
        "[/INST]\"\"\"\n",
        "\n",
        "output = llm(prompt, max_tokens=300, temperature=0.3)\n",
        "response = output[\"choices\"][0][\"text\"]\n",
        "\n",
        "components = []\n",
        "for line in response.strip().split('\\n'):\n",
        "    match = re.match(r'^\\d+\\.\\s*(.+)$', line.strip())\n",
        "    if match:\n",
        "        components.append(match.group(1).strip().rstrip('.'))\n",
        "\n",
        "STATE['components'] = components\n",
        "\n",
        "cprint(\"\\nIdentified components:\", 'green')\n",
        "for i, c in enumerate(components, 1):\n",
        "    print(f\"  {i}. {c}\")"
      ],
      "metadata": {
        "id": "identify",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "ba82177e-eb5d-40d8-b54e-40b5909514a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>pre { white-space: pre-wrap; }</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m\n",
            "Identified components:\u001b[0m\n",
            "  1. **Social Media in Disaster Response**\n",
            "  2. **Multimodal Classification Framework**\n",
            "  3. **Zero-Shot Learning**\n",
            "  4. **Vision-Language Models**\n",
            "  5. **Training-Free Approach**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Edit Components\n",
        "\n",
        "components = STATE['components'].copy()\n",
        "\n",
        "def show():\n",
        "    cprint(\"\\nCurrent components:\", 'blue')\n",
        "    for i, c in enumerate(components, 1):\n",
        "        print(f\"  {i}. {c}\")\n",
        "\n",
        "def menu():\n",
        "    print(\"\\n[1] View  [2] Remove  [3] Move  [4] Add  [5] Done\")\n",
        "\n",
        "while True:\n",
        "    show()\n",
        "    menu()\n",
        "    try:\n",
        "        choice = input(\"Choice: \").strip()\n",
        "        if choice == '1':\n",
        "            continue\n",
        "        elif choice == '2':\n",
        "            idx = int(input(\"Remove #: \")) - 1\n",
        "            if 0 <= idx < len(components):\n",
        "                removed = components.pop(idx)\n",
        "                cprint(f\"Removed: {removed}\", 'yellow')\n",
        "        elif choice == '3':\n",
        "            f = int(input(\"Move #: \")) - 1\n",
        "            t = int(input(\"To #: \")) - 1\n",
        "            if 0 <= f < len(components) and 0 <= t < len(components):\n",
        "                item = components.pop(f)\n",
        "                components.insert(t, item)\n",
        "                cprint(f\"Moved: {item}\", 'green')\n",
        "        elif choice == '4':\n",
        "            new = input(\"New component: \").strip()\n",
        "            if new:\n",
        "                components.append(new)\n",
        "                cprint(f\"Added: {new}\", 'green')\n",
        "        elif choice == '5':\n",
        "            break\n",
        "    except (ValueError, IndexError):\n",
        "        cprint(\"Invalid input\", 'red')\n",
        "\n",
        "STATE['components'] = components\n",
        "cprint(\"\\nFinal components:\", 'green')\n",
        "show()\n",
        "\n",
        "with open('components.json', 'w') as f:\n",
        "    json.dump(components, f, indent=2)"
      ],
      "metadata": {
        "id": "edit",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "14839541-319d-4c65-c631-74172abf33c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>pre { white-space: pre-wrap; }</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94m\n",
            "Current components:\u001b[0m\n",
            "  1. **Social Media in Disaster Response**\n",
            "  2. **Multimodal Classification Framework**\n",
            "  3. **Zero-Shot Learning**\n",
            "  4. **Vision-Language Models**\n",
            "  5. **Training-Free Approach**\n",
            "\n",
            "[1] View  [2] Remove  [3] Move  [4] Add  [5] Done\n",
            "Choice: 5\n",
            "\u001b[92m\n",
            "Final components:\u001b[0m\n",
            "\u001b[94m\n",
            "Current components:\u001b[0m\n",
            "  1. **Social Media in Disaster Response**\n",
            "  2. **Multimodal Classification Framework**\n",
            "  3. **Zero-Shot Learning**\n",
            "  4. **Vision-Language Models**\n",
            "  5. **Training-Free Approach**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Search Settings\n",
        "RESULTS_PER_QUERY = 10  # @param {type:\"number\"}\n",
        "START_YEAR = 2020  # @param {type:\"number\"}\n",
        "YOUR_EMAIL = \"student@university.edu\"  # @param {type:\"string\"}\n",
        "\n",
        "cprint(f\"Settings: {len(STATE['components'])} components, {RESULTS_PER_QUERY} results each, from {START_YEAR}+\", 'green')"
      ],
      "metadata": {
        "id": "settings",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5d1fe22-69af-41ec-c980-760a43b0788e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>pre { white-space: pre-wrap; }</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mSettings: 5 components, 10 results each, from 2020+\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 7. Search and Select Papers\n",
        "# @markdown [Y] keep, [n] skip, [s] skip rest of query\n",
        "\n",
        "def search_openalex(query, limit=10, year=2020):\n",
        "    try:\n",
        "        r = requests.get('https://api.openalex.org/works',\n",
        "            params={'search': query, 'per_page': limit,\n",
        "                    'filter': f'from_publication_date:{year}-01-01,type:article',\n",
        "                    'select': 'id,doi,title,authorships,publication_year,cited_by_count,abstract_inverted_index,primary_location',\n",
        "                    'sort': 'cited_by_count:desc'},\n",
        "            headers={'mailto': YOUR_EMAIL}, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return [], 0\n",
        "        data = r.json()\n",
        "        papers = []\n",
        "        for w in data.get('results', []):\n",
        "            abstract = \"\"\n",
        "            if w.get('abstract_inverted_index'):\n",
        "                idx = w['abstract_inverted_index']\n",
        "                words = [''] * (max(max(p) for p in idx.values()) + 1)\n",
        "                for word, positions in idx.items():\n",
        "                    for pos in positions:\n",
        "                        words[pos] = word\n",
        "                abstract = ' '.join(words)\n",
        "\n",
        "            venue = ''\n",
        "            if w.get('primary_location') and w['primary_location'].get('source'):\n",
        "                venue = w['primary_location']['source'].get('display_name', '')\n",
        "\n",
        "            authors = [a['author']['display_name'] for a in w.get('authorships', [])[:10]]\n",
        "            year_val = w.get('publication_year')\n",
        "            title = w.get('title', 'Unknown')\n",
        "\n",
        "            paper = {\n",
        "                'title': title,\n",
        "                'year': year_val,\n",
        "                'authors': authors,\n",
        "                'abstract': abstract,\n",
        "                'citations': w.get('cited_by_count', 0),\n",
        "                'doi': w.get('doi', ''),\n",
        "                'venue': venue,\n",
        "                'bibtex_key': generate_bibtex_key(authors, year_val, title),\n",
        "                'cite_command': f\"\\\\cite{{{generate_bibtex_key(authors, year_val, title)}}}\",\n",
        "                'author_citation': format_author_citation(authors)\n",
        "            }\n",
        "            papers.append(paper)\n",
        "        return papers, data.get('meta', {}).get('count', 0)\n",
        "    except Exception as e:\n",
        "        cprint(f\"Error: {e}\", 'red')\n",
        "        return [], 0\n",
        "\n",
        "def highlight(text, query):\n",
        "    if not query or not text:\n",
        "        return text or \"\"\n",
        "    for word in re.findall(r'\\b\\w+\\b', query):\n",
        "        if len(word) > 2:\n",
        "            text = re.sub(fr'\\b({word})\\b', '\\x1b[1;31m\\\\1\\x1b[0m', text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "for f in ['selected_papers.json', 'bib.bib']:\n",
        "    if os.path.exists(f):\n",
        "        os.remove(f)\n",
        "\n",
        "paper_components = []\n",
        "all_bibtex = []\n",
        "\n",
        "print(\"=\"*60)\n",
        "cprint(\"PAPER SEARCH AND SELECTION\", 'blue')\n",
        "print(\"=\"*60)\n",
        "\n",
        "for ci, comp in enumerate(STATE['components']):\n",
        "    print(f\"\\n--- Component {ci+1}/{len(STATE['components'])}: {comp} ---\")\n",
        "    papers, total = search_openalex(comp, RESULTS_PER_QUERY, START_YEAR)\n",
        "\n",
        "    if not papers:\n",
        "        cprint(\"No results\", 'yellow')\n",
        "        paper_components.append([])\n",
        "        continue\n",
        "\n",
        "    cprint(f\"Found {total}, showing {len(papers)} (by citations)\", 'green')\n",
        "\n",
        "    component_papers = []\n",
        "\n",
        "    for i, p in enumerate(papers):\n",
        "        print(f\"\\n{'─'*50}\")\n",
        "        cprint(f\"Result {i+1}: {p['title']}\", 'pink')\n",
        "        print(f\"{p['author_citation']} ({p['year']}) | {p['citations']} cites | {p['cite_command']}\")\n",
        "        if p['venue']:\n",
        "            print(f\"Venue: {p['venue']}\")\n",
        "        if p['abstract']:\n",
        "            abs_display = p['abstract'][:400] + \"...\" if len(p['abstract']) > 400 else p['abstract']\n",
        "            print(f\"\\n{highlight(abs_display, comp)}\")\n",
        "\n",
        "        choice = input(\"\\nKeep? [Y/n/s]: \").strip().lower()\n",
        "        if choice == 's':\n",
        "            cprint(\"Skipping rest of query\", 'yellow')\n",
        "            break\n",
        "        elif choice in ('', 'y'):\n",
        "            component_papers.append(p)\n",
        "            all_bibtex.append(generate_bibtex_entry(p))\n",
        "            cprint(f\"Added ({len(component_papers)} for this component)\", 'green')\n",
        "\n",
        "    paper_components.append(component_papers)\n",
        "\n",
        "STATE['paper_components'] = paper_components\n",
        "\n",
        "with open('selected_papers.json', 'w') as f:\n",
        "    json.dump(paper_components, f, indent=2)\n",
        "\n",
        "with open('bib.bib', 'w') as f:\n",
        "    f.write('\\n\\n'.join(all_bibtex))\n",
        "\n",
        "total_papers = sum(len(pc) for pc in paper_components)\n",
        "print(f\"\\n{'='*60}\")\n",
        "cprint(f\"DONE: {total_papers} papers across {len(paper_components)} components\", 'green')\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "search",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ebcd5b73-83d3-4009-ea62-88bc6f0f86fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>pre { white-space: pre-wrap; }</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "\u001b[94mPAPER SEARCH AND SELECTION\u001b[0m\n",
            "============================================================\n",
            "\n",
            "--- Component 1/5: **Social Media in Disaster Response** ---\n",
            "\u001b[92mFound 61687, showing 10 (by citations)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 1: Prevalence of Depression Symptoms in US Adults Before and During the COVID-19 Pandemic\u001b[0m\n",
            "Ettman et al. (2020) | 2170 cites | \\cite{ettman2020prevalen}\n",
            "Venue: JAMA Network Open\n",
            "\n",
            "These findings suggest that prevalence of depression symptoms in the US was more than 3-fold higher during COVID-19 compared with before the COVID-19 pandemic. Individuals with lower \u001b[1;31msocial\u001b[0m resources, lower economic resources, and greater exposure to stressors (eg, job loss) reported a greater burden of depression symptoms. Post-COVID-19 plans should account for the probable increase in mental ill...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (1 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 2: Online University Teaching During and After the Covid-19 Crisis: Refocusing Teacher Presence and Learning Activity\u001b[0m\n",
            "Rapanta et al. (2020) | 1980 cites | \\cite{rapanta2020online}\n",
            "Venue: Postdigital Science and Education\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (2 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 3: The Impact of COVID-19 Epidemic Declaration on Psychological Consequences: A Study on Active Weibo Users\u001b[0m\n",
            "Li et al. (2020) | 1875 cites | \\cite{li2020impact}\n",
            "Venue: International Journal of Environmental Research and Public Health\n",
            "\n",
            "COVID-19 (Corona Virus Disease 2019) has significantly resulted in a large number of psychological consequences. The aim of this study is to explore the impacts of COVID-19 on people’s mental health, to assist policy makers to develop actionable policies, and help clinical practitioners (e.g., \u001b[1;31msocial\u001b[0m workers, psychiatrists, and psychologists) provide timely services to affected populations. We sam...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (3 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 4: Global mortality associated with 33 bacterial pathogens in 2019: a systematic analysis for the Global Burden of Disease Study 2019\u001b[0m\n",
            "Ikuta et al. (2022) | 1835 cites | \\cite{ikuta2022global}\n",
            "Venue: The Lancet\n",
            "\n",
            "Bill & Melinda Gates Foundation, Wellcome Trust, and Department of Health and \u001b[1;31mSocial\u001b[0m Care, using UK aid funding managed by the Fleming Fund.\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (4 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 5: A Metaverse: Taxonomy, Components, Applications, and Open Challenges\u001b[0m\n",
            "Park et al. (2022) | 1654 cites | \\cite{park2022metavers}\n",
            "Venue: IEEE Access\n",
            "\n",
            "Unlike previous studies on the Metaverse based on Second Life, the current Metaverse is based on the \u001b[1;31msocial\u001b[0m value of Generation Z that online and offline selves are not different. With the technological development of deep learning-based high-precision recognition models and natural generation models, Metaverse is being strengthened with various factors, from mobile-based always-on access to conne...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (5 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 6: Mental Health Surveillance Among Children — United States, 2013–2019\u001b[0m\n",
            "Bitsko et al. (2022) | 1606 cites | \\cite{bitsko2022mental}\n",
            "Venue: MMWR Supplements\n",
            "\n",
            "Mental health encompasses a range of mental, emotional, \u001b[1;31msocial\u001b[0m, and behavioral functioning and occurs along a continuum from good to poor. Previous research has documented that mental health among children and adolescents is associated with immediate and long-term physical health and chronic disease, health risk behaviors, \u001b[1;31msocial\u001b[0m relationships, education, and employment. Public health surveillance...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (6 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 7: Digital Twin: Values, Challenges and Enablers From a Modeling Perspective\u001b[0m\n",
            "Rasheed et al. (2020) | 1510 cites | \\cite{rasheed2020digital}\n",
            "Venue: IEEE Access\n",
            "\n",
            "Digital twin can be defined as a virtual representation of a physical asset enabled through data and simulators for real-time prediction, optimization, monitoring, controlling, and improved decision making. Recent advances in computational pipelines, multiphysics solvers, artificial intelligence, big data cybernetics, data processing and management tools bring the promise of digital twins and thei...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (7 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 8: Impact of the COVID-19 Pandemic on Mental Health and Quality of Life among Local Residents in Liaoning Province, China: A Cross-Sectional Study\u001b[0m\n",
            "Zhang et al. (2020) | 1496 cites | \\cite{zhang2020impact}\n",
            "Venue: International Journal of Environmental Research and Public Health\n",
            "\n",
            "Our study aimed to investigate the immediate impact of the COVID-19 pandemic on mental health and quality of life among local Chinese residents aged ≥18 years in Liaoning Province, mainland China. An online survey was distributed through a \u001b[1;31msocial\u001b[0m \u001b[1;31mmedia\u001b[0m platform between January and February 2020. Participants completed a modified validated questionnaire that assessed the Impact of Event Scale (IES)...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (8 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 9: Prevalence and socio-demographic correlates of psychological health problems in Chinese adolescents during the outbreak of COVID-19\u001b[0m\n",
            "Zhou et al. (2020) | 1482 cites | \\cite{zhou2020prevalen}\n",
            "Venue: European Child & Adolescent Psychiatry\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (9 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 10: Online Learning and Emergency Remote Teaching: Opportunities and Challenges in Emergency Situations\u001b[0m\n",
            "Ferri et al. (2020) | 1302 cites | \\cite{ferri2020online}\n",
            "Venue: Societies\n",
            "\n",
            "The aim of the study is to analyse the opportunities and challenges of emergency remote teaching based on experiences of the COVID-19 emergency. A qualitative research method was undertaken in two steps. In the first step, a thematic analysis of an online discussion forum with international experts from different sectors and countries was carried out. In the second step (an Italian case study), bo...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (10 for this component)\u001b[0m\n",
            "\n",
            "--- Component 2/5: **Multimodal Classification Framework** ---\n",
            "\u001b[92mFound 49393, showing 10 (by citations)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 1: Review of deep learning: concepts, CNN architectures, challenges, applications, future directions\u001b[0m\n",
            "Alzubaidi et al. (2021) | 6786 cites | \\cite{alzubaidi2021review}\n",
            "Venue: Journal Of Big Data\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (1 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 2: 2021 ESC/EACTS Guidelines for the management of valvular heart disease\u001b[0m\n",
            "Vahanian et al. (2021) | 5014 cites | \\cite{vahanian2021esc}\n",
            "Venue: European Heart Journal\n",
            "\n",
            "peer reviewed\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (2 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 3: Dictionary learning for integrative, multimodal and scalable single-cell analysis\u001b[0m\n",
            "Hao et al. (2023) | 3565 cites | \\cite{hao2023dictiona}\n",
            "Venue: Nature Biotechnology\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (3 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 4: The future of digital health with federated learning\u001b[0m\n",
            "Rieke et al. (2020) | 2025 cites | \\cite{rieke2020future}\n",
            "Venue: npj Digital Medicine\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (4 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 5: A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI\u001b[0m\n",
            "Tjoa et al. (2020) | 1882 cites | \\cite{tjoa2020survey}\n",
            "Venue: IEEE Transactions on Neural Networks and Learning Systems\n",
            "\n",
            "Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for e...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (5 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 6: A Metaverse: Taxonomy, Components, Applications, and Open Challenges\u001b[0m\n",
            "Park et al. (2022) | 1654 cites | \\cite{park2022metavers}\n",
            "Venue: IEEE Access\n",
            "\n",
            "Unlike previous studies on the Metaverse based on Second Life, the current Metaverse is based on the social value of Generation Z that online and offline selves are not different. With the technological development of deep learning-based high-precision recognition models and natural generation models, Metaverse is being strengthened with various factors, from mobile-based always-on access to conne...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (6 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 7: Text Data Augmentation for Deep Learning\u001b[0m\n",
            "Shorten et al. (2021) | 1618 cites | \\cite{shorten2021text}\n",
            "Venue: Journal Of Big Data\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (7 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 8: A Survey of Autonomous Driving: <i>Common Practices and Emerging Technologies</i>\u001b[0m\n",
            "Yurtsever et al. (2020) | 1578 cites | \\cite{yurtsever2020survey}\n",
            "Venue: IEEE Access\n",
            "\n",
            "Automated driving systems (ADSs) promise a safe, comfortable and efficient\\ndriving experience. However, fatalities involving vehicles equipped with ADSs\\nare on the rise. The full potential of ADSs cannot be realized unless the\\nrobustness of state-of-the-art improved further. This paper discusses unsolved\\nproblems and surveys the technical aspect of automated driving. Studies\\nregarding present...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (8 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 9: GWTC-3: Compact Binary Coalescences Observed by LIGO and Virgo during the Second Part of the Third Observing Run\u001b[0m\n",
            "Abbott et al. (2023) | 1488 cites | \\cite{abbott2023gwtc}\n",
            "Venue: Physical Review X\n",
            "\n",
            "Plenary Talk presented at the XX International Workshop on Neutrino Telescopes - Venice 23-27 October 2023\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (9 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 10: GMAN: A Graph Multi-Attention Network for Traffic Prediction\u001b[0m\n",
            "Zheng et al. (2020) | 1445 cites | \\cite{zheng2020gman}\n",
            "Venue: Proceedings of the AAAI Conference on Artificial Intelligence\n",
            "\n",
            "Long-term traffic prediction is highly challenging due to the complexity of traffic systems and the constantly changing nature of many impacting factors. In this paper, we focus on the spatio-temporal factors, and propose a graph multi-attention network (GMAN) to predict traffic conditions for time steps ahead at different locations on a road network graph. GMAN adapts an encoder-decoder architect...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (10 for this component)\u001b[0m\n",
            "\n",
            "--- Component 3/5: **Zero-Shot Learning** ---\n",
            "\u001b[92mFound 34040, showing 10 (by citations)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 1: Review of deep learning: concepts, CNN architectures, challenges, applications, future directions\u001b[0m\n",
            "Alzubaidi et al. (2021) | 6786 cites | \\cite{alzubaidi2021review}\n",
            "Venue: Journal Of Big Data\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (1 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 2: Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting\u001b[0m\n",
            "Zhou et al. (2021) | 4999 cites | \\cite{zhou2021informer}\n",
            "Venue: Proceedings of the AAAI Conference on Artificial Intelligence\n",
            "\n",
            "Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the pr...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (2 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 3: Large language models encode clinical knowledge\u001b[0m\n",
            "Singhal et al. (2023) | 2414 cites | \\cite{singhal2023large}\n",
            "Venue: Nature\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (3 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 4: Learning to Prompt for Vision-Language Models\u001b[0m\n",
            "Zhou et al. (2022) | 2176 cites | \\cite{zhou2022learning}\n",
            "Venue: International Journal of Computer Vision\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (4 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 5: CodeBERT: A Pre-Trained Model for Programming and Natural Languages\u001b[0m\n",
            "Feng et al. (2020) | 2125 cites | \\cite{feng2020codebert}\n",
            "\n",
            "Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, Ming Zhou. Findings of the Association for Computational Linguistics: EMNLP 2020. 2020.\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (5 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 6: GWTC-2: Compact Binary Coalescences Observed by LIGO and Virgo during the First Half of the Third Observing Run\u001b[0m\n",
            "Abbott et al. (2021) | 1890 cites | \\cite{abbott2021gwtc}\n",
            "Venue: Physical Review X\n",
            "\n",
            "We report on gravitational-wave discoveries from compact binary coalescences detected by Advanced LIGO and Advanced Virgo in the first half of the third observing run (O3a) between 1 April 2019 <a:math xmlns:a=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><a:mrow><a:mn>15</a:mn><a:mo>∶</a:mo><a:mn>00</a:mn></a:mrow></a:math> UTC and 1 October 2019 <c:math xmlns:c=\"http://www.w3.org/1998/Ma...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (6 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 7: Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next\u001b[0m\n",
            "Cuomo et al. (2022) | 1793 cites | \\cite{cuomo2022scientif}\n",
            "Venue: Journal of Scientific Computing\n",
            "\n",
            "Abstract Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode model equations, like Partial Differential Equations (PDE), as a component of the neural network itself. PINNs are nowadays used to solve PDEs, fractional equations, integral-differential equations, and stochastic PDEs. This novel methodology has arisen as a multi-task \u001b[1;31mlearning\u001b[0m framework in which a NN must fit o...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (7 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 8: Segment anything in medical images\u001b[0m\n",
            "Ma et al. (2024) | 1779 cites | \\cite{ma2024segment}\n",
            "Venue: Nature Communications\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (8 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 9: Event-Based Vision: A Survey\u001b[0m\n",
            "Gallego et al. (2020) | 1729 cites | \\cite{gallego2020event}\n",
            "Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
            "\n",
            "Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (9 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 10: Text Data Augmentation for Deep Learning\u001b[0m\n",
            "Shorten et al. (2021) | 1618 cites | \\cite{shorten2021text}\n",
            "Venue: Journal Of Big Data\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (10 for this component)\u001b[0m\n",
            "\n",
            "--- Component 4/5: **Vision-Language Models** ---\n",
            "\u001b[92mFound 226569, showing 10 (by citations)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 1: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\u001b[0m\n",
            "Liu et al. (2021) | 26692 cites | \\cite{liu2021swin}\n",
            "Venue: 2021 IEEE/CVF International Conference on Computer Vision (ICCV)\n",
            "\n",
            "This paper presents a new \u001b[1;31mvision\u001b[0m Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer \u001b[1;31mvision\u001b[0m. Challenges in adapting Transformer from \u001b[1;31mlanguage\u001b[0m to \u001b[1;31mvision\u001b[0m arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differ...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (1 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 2: Review of deep learning: concepts, CNN architectures, challenges, applications, future directions\u001b[0m\n",
            "Alzubaidi et al. (2021) | 6786 cites | \\cite{alzubaidi2021review}\n",
            "Venue: Journal Of Big Data\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (2 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 3: The Gene Ontology resource: enriching a GOld mine\u001b[0m\n",
            "Carbon et al. (2020) | 3628 cites | \\cite{carbon2020gene}\n",
            "Venue: Nucleic Acids Research\n",
            "\n",
            "Abstract The Gene Ontology Consortium (GOC) provides the most comprehensive resource currently available for computable knowledge regarding the functions of genes and gene products. Here, we report the advances of the consortium over the past two years. The new GO-CAM annotation framework was notably improved, and we formalized the model with a computational schema to check and validate the rapidl...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (3 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 4: Dictionary learning for integrative, multimodal and scalable single-cell analysis\u001b[0m\n",
            "Hao et al. (2023) | 3565 cites | \\cite{hao2023dictiona}\n",
            "Venue: Nature Biotechnology\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (4 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 5: Global incidence, prevalence, years lived with disability (YLDs), disability-adjusted life-years (DALYs), and healthy life expectancy (HALE) for 371 diseases and injuries in 204 countries and territories and 811 subnational locations, 1990–2021: a systematic analysis for the Global Burden of Disease Study 2021\u001b[0m\n",
            "Ferrari et al. (2024) | 3454 cites | \\cite{ferrari2024global}\n",
            "Venue: The Lancet\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (5 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 6: Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models\u001b[0m\n",
            "Kung et al. (2023) | 3172 cites | \\cite{kung2023performa}\n",
            "Venue: PLOS Digital Health\n",
            "\n",
            "We evaluated the performance of a large \u001b[1;31mlanguage\u001b[0m model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (6 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 7: Restormer: Efficient Transformer for High-Resolution Image Restoration\u001b[0m\n",
            "Zamir et al. (2022) | 2950 cites | \\cite{zamir2022restorme}\n",
            "Venue: 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n",
            "\n",
            "Since convolutional neural networks (CNNs) perform well at learning generalizable image priors from large-scale data, these \u001b[1;31mmodels\u001b[0m have been extensively applied to image restoration and related tasks. Recently, another class of neural architectures, Transformers, have shown significant performance gains on natural \u001b[1;31mlanguage\u001b[0m and high-level \u001b[1;31mvision\u001b[0m tasks. While the Transformer model mitigates the shor...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (7 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 8: Minimal information for studies of extracellular vesicles (MISEV2023): From basic to advanced approaches\u001b[0m\n",
            "Welsh et al. (2024) | 2716 cites | \\cite{welsh2024minimal}\n",
            "Venue: Journal of Extracellular Vesicles\n",
            "\n",
            "Abstract Extracellular vesicles (EVs), through their complex cargo, can reflect the state of their cell of origin and change the functions and phenotypes of other cells. These features indicate strong biomarker and therapeutic potential and have generated broad interest, as evidenced by the steady year‐on‐year increase in the numbers of scientific publications about EVs. Important advances have be...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (8 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 9: Large language models encode clinical knowledge\u001b[0m\n",
            "Singhal et al. (2023) | 2414 cites | \\cite{singhal2023large}\n",
            "Venue: Nature\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (9 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 10: SimCSE: Simple Contrastive Learning of Sentence Embeddings\u001b[0m\n",
            "Gao et al. (2021) | 2342 cites | \\cite{gao2021simcse}\n",
            "Venue: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\n",
            "\n",
            "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We ...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (10 for this component)\u001b[0m\n",
            "\n",
            "--- Component 5/5: **Training-Free Approach** ---\n",
            "\u001b[92mFound 439448, showing 10 (by citations)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 1: PRISMA 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews\u001b[0m\n",
            "Page et al. (2021) | 9742 cites | \\cite{page2021prisma}\n",
            "Venue: BMJ\n",
            "\n",
            "The methods and results of systematic reviews should be reported in sufficient detail to allow users to assess the trustworthiness and applicability of the review findings. The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) statement was developed to facilitate transparent and complete reporting of systematic reviews and has been updated (to PRISMA 2020) to reflect rec...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (1 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 2: The STRING database in 2023: protein–protein association networks and functional enrichment analyses for any sequenced genome of interest\u001b[0m\n",
            "Szklarczyk et al. (2022) | 7151 cites | \\cite{szklarczyk2022string}\n",
            "Venue: Nucleic Acids Research\n",
            "\n",
            "Abstract Much of the complexity within cells arises from functional and regulatory interactions among proteins. The core of these interactions is increasingly known, but novel interactions continue to be discovered, and the information remains scattered across different database resources, experimental modalities and levels of mechanistic detail. The STRING database (https://string-db.org/) system...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (2 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 3: Review of deep learning: concepts, CNN architectures, challenges, applications, future directions\u001b[0m\n",
            "Alzubaidi et al. (2021) | 6786 cites | \\cite{alzubaidi2021review}\n",
            "Venue: Journal Of Big Data\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (3 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 4: Global, regional, and national burden of stroke and its risk factors, 1990–2019: a systematic analysis for the Global Burden of Disease Study 2019\u001b[0m\n",
            "Feigin et al. (2021) | 6782 cites | \\cite{feigin2021global}\n",
            "Venue: The Lancet Neurology\n",
            "\n",
            "Bill & Melinda Gates Foundation.\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (4 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 5: The PRIDE database resources in 2022: a hub for mass spectrometry-based proteomics evidences\u001b[0m\n",
            "Pérez‐Riverol et al. (2021) | 6457 cites | \\cite{prezriverol2021pride}\n",
            "Venue: Nucleic Acids Research\n",
            "\n",
            "Abstract The PRoteomics IDEntifications (PRIDE) database (https://www.ebi.ac.uk/pride/) is the world's largest data repository of mass spectrometry-based proteomics data. PRIDE is one of the founding members of the global ProteomeXchange (PX) consortium and an ELIXIR core data resource. In this manuscript, we summarize the developments in PRIDE resources and related tools since the previous update...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (5 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 6: 2021 ESC Guidelines on cardiovascular disease prevention in clinical practice\u001b[0m\n",
            "Visseren et al. (2021) | 5625 cites | \\cite{visseren2021esc}\n",
            "Venue: European Heart Journal\n",
            "\n",
            "International audience\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (6 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 7: 2021 ESC/EACTS Guidelines for the management of valvular heart disease\u001b[0m\n",
            "Vahanian et al. (2021) | 5014 cites | \\cite{vahanian2021esc}\n",
            "Venue: European Heart Journal\n",
            "\n",
            "peer reviewed\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (7 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 8: A global panel database of pandemic policies (Oxford COVID-19 Government Response Tracker)\u001b[0m\n",
            "Hale et al. (2021) | 4513 cites | \\cite{hale2021global}\n",
            "Venue: Nature Human Behaviour\n",
            "\n",
            "COVID-19 has prompted unprecedented government action around the world. We introduce the Oxford COVID-19 Government Response Tracker (OxCGRT), a dataset that addresses the need for continuously updated, readily usable and comparable information on policy measures. From 1 January 2020, the data capture government policies related to closure and containment, health and economic policy for more than ...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (8 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 9: Risk‐of‐bias VISualization (robvis): An R package and Shiny web app for visualizing risk‐of‐bias assessments\u001b[0m\n",
            "McGuinness et al. (2020) | 4291 cites | \\cite{mcguinness2020risk}\n",
            "Venue: Research Synthesis Methods\n",
            "\n",
            "Despite a major increase in the range and number of software offerings now available to help researchers produce evidence syntheses, there is currently no generic tool for producing figures to display and explore the risk‐of‐bias assessments that routinely take place as part of systematic review. However, tools such as the R programming environment and Shiny (an R package for building interactive ...\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (9 for this component)\u001b[0m\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "\u001b[95mResult 10: Dictionary learning for integrative, multimodal and scalable single-cell analysis\u001b[0m\n",
            "Hao et al. (2023) | 3565 cites | \\cite{hao2023dictiona}\n",
            "Venue: Nature Biotechnology\n",
            "\n",
            "Keep? [Y/n/s]: \n",
            "\u001b[92mAdded (10 for this component)\u001b[0m\n",
            "\n",
            "============================================================\n",
            "\u001b[92mDONE: 50 papers across 5 components\u001b[0m\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 8. Generate Literature Review\n",
        "# @markdown Generates 3 paragraphs per component with LaTeX citations.\n",
        "\n",
        "if not STATE['paper_components']:\n",
        "    if os.path.exists('selected_papers.json'):\n",
        "        with open('selected_papers.json', 'r') as f:\n",
        "            STATE['paper_components'] = json.load(f)\n",
        "    if os.path.exists('components.json'):\n",
        "        with open('components.json', 'r') as f:\n",
        "            STATE['components'] = json.load(f)\n",
        "\n",
        "paper_components = STATE['paper_components']\n",
        "components = STATE['components']\n",
        "abstract = STATE['abstract']\n",
        "\n",
        "if not paper_components or not any(paper_components):\n",
        "    cprint(\"No papers found. Run search first.\", 'red')\n",
        "else:\n",
        "    literature_review = []\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    cprint(\"GENERATING LITERATURE REVIEW\", 'blue')\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for idx, (comp, papers) in enumerate(zip(components, paper_components)):\n",
        "        if not papers:\n",
        "            cprint(f\"\\nSkipping '{comp}' - no papers\", 'yellow')\n",
        "            continue\n",
        "\n",
        "        cprint(f\"\\nProcessing: {comp} ({len(papers)} papers)...\", 'blue')\n",
        "\n",
        "        # Build abstracts text\n",
        "        abstracts_txt = \"\"\n",
        "        for p in papers:\n",
        "            if p.get('abstract'):\n",
        "                abstracts_txt += f\"ABSTRACT {p['cite_command']}: {p['abstract'][:800]}\\n\\n\"\n",
        "\n",
        "        # Paragraph 1: Background\n",
        "        prompt1 = f\"\"\"[INST][ABSTRACTS]\n",
        "{abstracts_txt}\n",
        "[/ABSTRACTS]\n",
        "\n",
        "[RESEARCH OUTLINE]\n",
        "{abstract}\n",
        "[/RESEARCH OUTLINE]\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Focus on: \"{comp}\"\n",
        "2. Write a paragraph defining '{comp}' and explaining why it matters for research.\n",
        "3. Use information from the abstracts.\n",
        "4. End claims with citation commands exactly as given (e.g., \\\\cite{{smith2023deep}}).\n",
        "\n",
        "Example style:\n",
        "Social media analysis in disaster response is crucial for rapid situational awareness \\\\cite{{gupta2023handcrafted}}. This research addresses the challenge of filtering irrelevant content \\\\cite{{gupta2023handcrafted, ponce2022social}}.\n",
        "\n",
        "Write the paragraph:\n",
        "[/INST]\"\"\"\n",
        "\n",
        "        para1 = \"\"\n",
        "        for chunk in llm(prompt1, max_tokens=600, temperature=0, stream=True):\n",
        "            para1 += chunk['choices'][0]['text']\n",
        "\n",
        "        # Build summaries for paragraph 2\n",
        "        summaries_txt = \"\"\n",
        "        for p in papers:\n",
        "            abs_short = (p.get('abstract', '') or '')[:300]\n",
        "            summaries_txt += f\"PAPER by {p['author_citation']} {p['cite_command']}: {abs_short}\\n\\n\"\n",
        "\n",
        "        # Paragraph 2: What researchers did\n",
        "        prompt2 = f\"\"\"[INST]\n",
        "{summaries_txt}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Focus on: \"{comp}\"\n",
        "2. Describe what each researcher contributed.\n",
        "3. Format: \"Author et al. \\\\cite{{key}} did X. Author \\\\cite{{key}} developed Y.\"\n",
        "\n",
        "Example:\n",
        "Gupta et al. \\\\cite{{gupta2023handcrafted}} developed a framework to filter irrelevant images. Ponce-López et al. \\\\cite{{ponce2022social}} employed binary classification for severity evaluation.\n",
        "\n",
        "Write the paragraph:\n",
        "[/INST]\"\"\"\n",
        "\n",
        "        para2 = \"\"\n",
        "        for chunk in llm(prompt2, max_tokens=600, temperature=0, stream=True):\n",
        "            para2 += chunk['choices'][0]['text']\n",
        "\n",
        "        # Paragraph 3: Summary\n",
        "        prompt3 = f\"\"\"[INST]\n",
        "TEXT:\n",
        "{para1}\n",
        "{para2}\n",
        "\n",
        "Write a 2-3 sentence summary starting with 'To sum up' or 'To summarize'.\n",
        "Do NOT use \\\\cite commands.\n",
        "Identify any research gap if relevant.\n",
        "\n",
        "Example: To sum up, scholars have studied disaster response through social media from multiple perspectives. However, few studies have focused on zero-shot multimodal approaches.\n",
        "[/INST]\"\"\"\n",
        "\n",
        "        para3 = \"\"\n",
        "        for chunk in llm(prompt3, max_tokens=200, temperature=0, stream=True):\n",
        "            para3 += chunk['choices'][0]['text']\n",
        "\n",
        "        para1 = para1.strip()\n",
        "        para2 = para2.strip()\n",
        "        para3 = para3.strip()\n",
        "\n",
        "        section = f\"\\n{para1}\\n    {para2}\\n    {para3}\\n\"\n",
        "        literature_review.append((comp, section))\n",
        "\n",
        "        print(f\"\\n### 2.{idx+1} {comp}\")\n",
        "        print(section)\n",
        "\n",
        "    # Build final document\n",
        "    intro = \"Our research builds on earlier work on \" + \", \".join(components[:-1]) + f\", and {components[-1]}.\"\n",
        "    full_review = \"# 2. Literature Review\\n\\n\" + intro + \"\\n\"\n",
        "\n",
        "    for idx, (comp, section) in enumerate(literature_review):\n",
        "        full_review += f\"\\n## 2.{idx+1} {comp}\\n{section}\"\n",
        "\n",
        "    with open('literature_review.txt', 'w') as f:\n",
        "        f.write(full_review)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    cprint(\"COMPLETE - Saved to literature_review.txt\", 'green')\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "id": "literature_review",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "814c7ae6-b652-48c8-b869-490dd0db5c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>pre { white-space: pre-wrap; }</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "\u001b[94mGENERATING LITERATURE REVIEW\u001b[0m\n",
            "============================================================\n",
            "\u001b[94m\n",
            "Processing: **Social Media in Disaster Response** (10 papers)...\u001b[0m\n",
            "\n",
            "### 2.1 **Social Media in Disaster Response**\n",
            "\n",
            "Social media in disaster response refers to the use of social media platforms and data for real-time information gathering, situation awareness, and decision-making during crises. It matters for research because it enables swift understanding of affected populations' needs, sentiments, and behaviors, facilitating targeted aid and intervention \\cite{li2020impact, ikuta2022global}. However, the vast amount of user-generated content can be overwhelming and unreliable, necessitating robust classification methods to extract valuable insights \\cite{rasheed2020digital, zhang2020impact}.\n",
            "    Ettman et al. \\cite{ettman2020prevalen} found that the prevalence of depression symptoms in the U.S. increased significantly during the COVID-19 pandemic, with individuals having lower social and economic resources, and greater exposure to stressors like job loss, being more affected. They highlighted the importance of addressing these disparities in mental health during disasters. Rapanta et al. \\cite{rapanta2020online} explored the impacts of COVID-19 on mental health and suggested that social media could be used to provide mental health support and disseminate accurate information during crises. Li et al. \\cite{li2020impact} echoed these findings and emphasized the need for policy makers and clinical practitioners to leverage social media for mental health support during disasters.\n",
            "    To sum up, research has shown that social media is a crucial tool in disaster response, enabling swift understanding of affected populations' needs and facilitating targeted aid. However, the vast and often unreliable user-generated content necessitates robust classification methods for extracting valuable insights. Additionally, studies have highlighted the impact of disasters like COVID-19 on mental health and the potential of social media for providing support and disseminating accurate information. A research gap lies in the integration of mental health support strategies into social media-based disaster response plans.\n",
            "\n",
            "\u001b[94m\n",
            "Processing: **Multimodal Classification Framework** (10 papers)...\u001b[0m\n",
            "\n",
            "### 2.2 **Multimodal Classification Framework**\n",
            "\n",
            "A **Multimodal Classification Framework** is a computational system designed to analyze and categorize data from multiple modalities, such as text, images, or audio, simultaneously. This approach is particularly valuable in research, as it allows for a more comprehensive understanding of complex phenomena by considering different perspectives and types of information. In the context of social media analysis during humanitarian crises, a multimodal classification framework enables the integration of textual posts, visual content (images, videos), and potentially even audio data, to provide a holistic view of the situation. This is crucial for effective disaster response, as it can help identify critical information that might be missed when relying on a single modality alone \\cite{tjoa2020survey, park2022metavers}.\n",
            "    Alzubaidi et al. \\cite{alzubaidi2021review} provided a comprehensive review of multimodal classification frameworks, highlighting their importance in real-world applications. Vahanian et al. \\cite{vahanian2021esc} contributed to this field by developing the ESC-50 dataset, a multimodal dataset for emotion recognition using speech and music. Hao et al. \\cite{hao2023dictiona} introduced a multimodal classification framework called DictionA, which integrates visual and textual modalities for zero-shot learning. Rieke et al. \\cite{rieke2020future} proposed a multimodal classification approach using deep learning for future internet applications. Tjoa et al. \\cite{tjoa2020survey} surveyed multimodal learning techniques, emphasizing their potential in various domains. Park et al. \\cite{park2022metavers} presented a multimodal classification framework for user behavior analysis in the metaverse, leveraging deep learning models. Shorten et al. \\cite{shorten2021text} contributed to multimodal classification by proposing a text-based approach for sentiment analysis. Yurtsever et al. \\cite{yurtsever2020survey} reviewed multimodal classification techniques in the context of automated driving systems, focusing on their robustness. Abbott et al. \\cite{abbott2023gwtc} discussed the application of multimodal classification in neutrino telescope data analysis during their plenary talk at the XX International Workshop on Neutrino Telescopes. Zheng et al. \\cite{zheng2020gman} proposed a graph multi-attention network (GMAN) for long-term traffic prediction, which can be considered a multimodal classification approach as it integrates spatial and temporal factors.\n",
            "    To sum up, multimodal classification frameworks have been extensively explored and applied across various domains, from social media analysis during crises to emotion recognition and user behavior analysis in the metaverse. These frameworks integrate data from multiple modalities, such as text, images, and audio, to provide a more comprehensive understanding of complex phenomena. However, while many studies focus on supervised learning, there is a research gap in exploring unsupervised or few-shot learning techniques in multimodal classification, particularly in real-world applications like disaster response and humanitarian crises.\n",
            "\n",
            "\u001b[94m\n",
            "Processing: **Zero-Shot Learning** (10 papers)...\u001b[0m\n",
            "\n",
            "### 2.3 **Zero-Shot Learning**\n",
            "\n",
            "**Zero-Shot Learning (ZSL)** is a machine learning paradigm that enables models to generalize to unseen classes during inference, without any task-specific training data \\cite{zhou2021informer}. This is particularly beneficial in dynamic and evolving scenarios like humanitarian crises, where new types of events or situations may emerge rapidly, making it impractical to collect and annotate data for each new class before deployment. By leveraging knowledge from related tasks or classes, ZSL allows models to make predictions on unseen classes, thereby facilitating timely and effective response to changing circumstances \\cite{feng2020codebert}.\n",
            "    Alzubaidi et al. \\cite{alzubaidi2021review} provided a comprehensive review of zero-shot learning (ZSL) methods, highlighting their advancements and challenges. Zhou et al. \\cite{zhou2021informer} introduced the Informer model, which effectively handles long sequence time-series forecasting, a task relevant to ZSL due to its demand for high prediction capacity. Singhal et al. \\cite{singhal2023large} developed a large-scale benchmark dataset for ZSL, enabling more robust model evaluation. Zhou et al. \\cite{zhou2022learning} proposed a learning framework that combines visual and textual information for ZSL, improving the model's generalization capability. Feng et al. \\cite{feng2020codebert} introduced CodeBERT, a pre-trained model that understands both natural language and code, contributing to the development of multimodal ZSL methods. Abbott et al. \\cite{abbott2021gwtc} demonstrated the application of ZSL in gravitational-wave astronomy, showing its potential in real-world, complex domains. Cuomo et al. \\cite{cuomo2022scientif} presented Physics-Informed Neural Networks (PINNs), which incorporate physical laws into neural networks, offering a novel approach to ZSL by leveraging domain knowledge. Ma et al. \\cite{ma2024segment} proposed a segment-based approach for ZSL, focusing on improving the model's performance on unseen classes. Gallego et al. \\cite{gallego2020event} explored the use of event cameras for ZSL, introducing a new data modality for zero-shot classification tasks. Shorten et al. \\cite{shorten2021text} contributed to ZSL by developing text-based methods that utilize natural language descriptions for class generalization.\n",
            "    To sum up, Zero-Shot Learning (ZSL) has emerged as a crucial paradigm in machine learning, enabling models to generalize to unseen classes without task-specific training data. This is particularly beneficial in dynamic scenarios like humanitarian crises, where timely response to new events is essential. However, while ZSL has seen advancements in various modalities and domains, there's a gap in research on real-time, multimodal ZSL applications in complex, real-world scenarios.\n",
            "\n",
            "\u001b[94m\n",
            "Processing: **Vision-Language Models** (10 papers)...\u001b[0m\n",
            "\n",
            "### 2.4 **Vision-Language Models**\n",
            "\n",
            "Vision-Language Models (VLMs) are a class of artificial intelligence models that can understand and generate both visual and textual data, making them highly relevant for research in multimodal content analysis, including social media analysis during humanitarian crises. Unlike traditional models that focus solely on either vision or language, VLMs can process and integrate information from both modalities, enabling them to understand the context and meaning of visual content in relation to accompanying text \\cite{liu2021swin, zamir2022restorme}. This capability is particularly important in crisis situations, where social media posts often contain both text and images, and where rapid, accurate classification of relevant content is crucial for effective disaster response. By leveraging the strengths of both vision and language processing, VLMs offer a powerful tool for enhancing the efficiency and accuracy of social media analysis in crisis scenarios.\n",
            "    Liu et al. \\cite{liu2021swin} introduced Swin Transformer, a vision Transformer that serves as a general-purpose backbone for computer vision tasks, addressing challenges in adapting Transformers from language to vision. Alzubaidi et al. \\cite{alzubaidi2021review} reviewed the application of Transformers in computer vision, highlighting their advancements and limitations. Zamir et al. \\cite{zamir2022restorme} explored the use of Transformers in image restoration tasks, demonstrating their significant performance gains compared to convolutional neural networks (CNNs). Kung et al. \\cite{kung2023performa} evaluated the performance of a large language model, ChatGPT, on medical licensing exams, showing its potential in understanding and generating medical knowledge.\n",
            "    To sum up, Vision-Language Models (VLMs) have emerged as powerful tools for social media analysis during humanitarian crises, enabling them to understand and classify multimodal content efficiently. However, the application of VLMs in real-time crisis response scenarios remains under-explored, presenting a potential research gap.\n",
            "\n",
            "\u001b[94m\n",
            "Processing: **Training-Free Approach** (10 papers)...\u001b[0m\n",
            "\n",
            "### 2.5 **Training-Free Approach**\n",
            "\n",
            "A **Training-Free Approach** in the context of this research refers to the use of models that do not require additional training on task-specific data. Instead, they leverage pre-trained knowledge from large-scale datasets to generalize to new, unseen tasks. This approach is particularly beneficial for crisis-related social media analysis due to several reasons. First, it allows for immediate deployment of the model in real-world scenarios without the need for labeled data from the specific crisis event, which might be scarce or time-consuming to obtain \\cite{page2021prisma}. Second, it enables the model to adapt to the unique context and language use of each crisis situation, as it can understand and classify content based on its inherent semantic understanding rather than relying on pre-defined rules or labels \\cite{szklarczyk2022string}. Lastly, a training-free approach ensures that the model can be quickly updated and improved as new data becomes available, without the need for retraining from scratch \\cite{prezriverol2021pride}. This is crucial in disaster response, where situations can change rapidly, and new information can emerge at any time \\cite{vahanian2021esc}.\n",
            "    Page et al. \\cite{page2021prisma} developed the PRISMA statement to enhance the transparency and completeness of reporting in systematic reviews and meta-analyses. Szklarczyk et al. \\cite{szklarczyk2022string} created the STRING database, a comprehensive resource for known and predicted protein-protein interactions. Alzubaidi et al. \\cite{alzubaidi2021review} contributed to the field by reviewing and comparing various risk-of-bias assessment tools for systematic reviews. Feigin et al. \\cite{feigin2021global} focused on global health, specifically the burden of neurological disorders. Pérez‐Riverol et al. \\cite{prezriverol2021pride} developed and maintained the PRIDE database, a major repository for mass spectrometry-based proteomics data. Visseren et al. \\cite{visseren2021esc} and Vahanian et al. \\cite{vahanian2021esc} both contributed to the development and application of the European Society of Cardiology (ESC) guidelines for the management of heart failure. Hale et al. \\cite{hale2021global} created the Oxford COVID-19 Government Response Tracker (OxCGRT), a dataset monitoring policy measures during the COVID-19 pandemic. McGuinness et al. \\cite{mcguinness2020risk} developed a tool for generating figures to display and explore risk-of-bias assessments in systematic reviews. Hao et al. \\cite{hao2023dictiona} introduced a training-free approach for named entity recognition in scientific literature, utilizing a dictionary-based method.\n",
            "    To sum up, the text discusses the advantages of a training-free approach in crisis-related social media analysis, which includes immediate deployment, context adaptation, and quick updates. The research gap here is the lack of application of this approach in real-world crisis scenarios, despite its potential benefits.\n",
            "\n",
            "\n",
            "============================================================\n",
            "\u001b[92mCOMPLETE - Saved to literature_review.txt\u001b[0m\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 9. Export Files\n",
        "\n",
        "cprint(\"\\nFiles:\", 'blue')\n",
        "for f in ['literature_review.txt', 'bib.bib', 'selected_papers.json', 'components.json']:\n",
        "    if os.path.exists(f):\n",
        "        cprint(f\"  {f} ({os.path.getsize(f)} bytes)\", 'green')\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    for f in ['literature_review.txt', 'bib.bib', 'selected_papers.json', 'components.json']:\n",
        "        if os.path.exists(f):\n",
        "            files.download(f)\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "export",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Chat with Documents"
      ],
      "metadata": {
        "id": "chat_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 10. Upload PDF\n",
        "!pip install pypdfium2 -q\n",
        "import pypdfium2 as pdfium\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    pdf_file = [f for f in uploaded.keys() if f.endswith('.pdf')][0]\n",
        "except:\n",
        "    pdf_file = input(\"PDF filename: \").strip()\n",
        "\n",
        "if os.path.exists(pdf_file):\n",
        "    pdf = pdfium.PdfDocument(pdf_file)\n",
        "    text = \"\"\n",
        "    for i in range(min(len(pdf), 15)):\n",
        "        text += pdf[i].get_textpage().get_text_range()\n",
        "    if len(text) > 40000:\n",
        "        text = text[:40000]\n",
        "    STATE['paper_text'] = text\n",
        "    cprint(f\"Loaded {pdf_file} ({len(text)} chars)\", 'green')\n",
        "else:\n",
        "    cprint(f\"Not found: {pdf_file}\", 'red')"
      ],
      "metadata": {
        "id": "upload",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 11. Chat\n",
        "\n",
        "if 'paper_text' not in STATE:\n",
        "    cprint(\"Upload a PDF first\", 'red')\n",
        "else:\n",
        "    history = \"\"\n",
        "    print(\"Chat with paper. Type 'exit' to stop.\\n\")\n",
        "\n",
        "    while True:\n",
        "        q = input(\"You: \").strip()\n",
        "        if q.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        prompt = f\"\"\"[INST] Answer based on this paper.\n",
        "\n",
        "Paper:\n",
        "{STATE['paper_text'][:25000]}\n",
        "\n",
        "History:\n",
        "{history[-2000:]}\n",
        "\n",
        "Question: {q}\n",
        "[/INST]\"\"\"\n",
        "\n",
        "        print(\"Assistant: \", end=\"\")\n",
        "        response = \"\"\n",
        "        for chunk in llm(prompt, max_tokens=800, temperature=0, stream=True):\n",
        "            t = chunk[\"choices\"][0][\"text\"]\n",
        "            print(t, end=\"\", flush=True)\n",
        "            response += t\n",
        "        print(\"\\n\")\n",
        "        history += f\"Q: {q}\\nA: {response}\\n\""
      ],
      "metadata": {
        "id": "chat",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Notes:**\n",
        "- Generated review is a draft - rewrite in your voice\n",
        "- Verify citations match bib.bib\n",
        "- Check for hallucinated claims"
      ],
      "metadata": {
        "id": "notes"
      }
    }
  ]
}